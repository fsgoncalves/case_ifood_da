{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d65763",
   "metadata": {},
   "source": [
    "## Configurações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9497d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_projeto = \"D:\\\\__case_ifood\"\n",
    "\n",
    "# Caminho do arquivo JSON da conta de serviço\n",
    "credencial_gcp = os.path.join(pasta_projeto,\"case-ifood-fsg-6f1d7cf34e08.json\")\n",
    "\n",
    "# Autenticando\n",
    "credencial = service_account.Credentials.from_service_account_file(credencial_gcp)\n",
    "\n",
    "# Cliente BigQuery\n",
    "client = bigquery.Client(credentials=credencial, project=\"case-ifood-fsg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2556fb",
   "metadata": {},
   "source": [
    "### Função para Criar o dataset e inserir a tabela no BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size):\n",
    "    \"\"\"Divide um DataFrame Pandas em pedaços menores.\"\"\"\n",
    "    return [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "def criar_dataset_e_tabela_append(df, dataset_nome, tabela_nome, client, project_id, location=\"southamerica-east1\", chunk_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Cria um dataset (se necessário) e insere os dados em uma tabela BigQuery usando chunks.\n",
    "    Sempre faz append (WRITE_APPEND). Suporta DataFrames do Pandas ou Polars.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_id = f\"{project_id}.{dataset_nome}\"\n",
    "    table_id = f\"{dataset_id}.{tabela_nome}\"\n",
    "\n",
    "    # 1. Criar dataset se não existir\n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = location\n",
    "        client.create_dataset(dataset, exists_ok=True)\n",
    "        print(f\"Dataset '{dataset_id}' pronto.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Converter de Polars para Pandas, se necessário\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = df.to_pandas()\n",
    "        print(\"Convertido de Polars para Pandas.\")\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"O objeto fornecido não é um DataFrame válido.\")\n",
    "        return\n",
    "\n",
    "    # 3. Inserir em chunks (sempre com WRITE_APPEND)\n",
    "    try:\n",
    "        chunks = split_dataframe(df, chunk_size)\n",
    "        total_rows = 0\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Enviando chunk {i+1}/{len(chunks)} com {len(chunk)} linhas...\")\n",
    "            job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "            job = client.load_table_from_dataframe(chunk, table_id, job_config=job_config)\n",
    "            job.result()\n",
    "            total_rows += len(chunk)\n",
    "\n",
    "        print(f\"Tabela '{table_id}' atualizada com {total_rows} linhas (append).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar tabela por chunks: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a33c87",
   "metadata": {},
   "source": [
    "### Função para enviar os arquivos Parquet para o BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f145e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enviar_parquets_para_bigquery(pasta_projeto, dataset_nome, tabela_nome, client, project_id, location=\"southamerica-east1\"):\n",
    "    \"\"\"\n",
    "    Percorre os arquivos 'sales_*.parquet' na pasta_projeto e faz append no BigQuery.\n",
    "    \"\"\"\n",
    "    arquivos = sorted([arq for arq in os.listdir(pasta_projeto) if arq.startswith(\"sales_\") and arq.endswith(\".parquet\")])\n",
    "\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum arquivo 'sales_*.parquet' encontrado na pasta.\")\n",
    "        return\n",
    "\n",
    "    for arquivo in arquivos:\n",
    "        caminho_arquivo = os.path.join(pasta_projeto, arquivo)\n",
    "        print(f\"Lendo arquivo: {arquivo}\")\n",
    "\n",
    "        try:\n",
    "            df = pl.read_parquet(caminho_arquivo)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {arquivo}: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Enviando para BigQuery: {arquivo}\")\n",
    "        criar_dataset_e_tabela_append(\n",
    "            df=df,\n",
    "            dataset_nome=dataset_nome,\n",
    "            tabela_nome=tabela_nome,\n",
    "            client=client,\n",
    "            project_id=project_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a1a02",
   "metadata": {},
   "source": [
    "#### Filtro para otimizar o download da tabela no BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "semanas = [\n",
    "    \"2018-12-02\", \"2018-12-09\", \"2018-12-16\", \"2018-12-23\",\n",
    "    \"2018-12-30\", \"2019-01-06\", \"2019-01-13\", \"2019-01-20\", \"2019-01-27\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe78874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando o download da tabela e salvando em diversos arquivos parquet\n",
    "\n",
    "# df_sales_chunk = []\n",
    "\n",
    "for semana in semanas:\n",
    "    print(f\"Lendo semana: {semana}\")\n",
    "\n",
    "    qry = f\"\"\"\n",
    "        WITH tabela AS (\n",
    "            SELECT \n",
    "                o.order_id,\n",
    "                o.order_created_at,\n",
    "                od.name AS product,\n",
    "                od.quantity,\n",
    "                od.unitPrice,\n",
    "                od.addition,\n",
    "                od.discount,\n",
    "                o.order_total_amount,\n",
    "                od.type AS product_type,\n",
    "                od.sequence,\n",
    "                o.customer_id,\n",
    "                o.cpf,\n",
    "                COALESCE(c.customer_name, o.customer_name) AS customer_name,\n",
    "                c.created_at AS customer_created_at,\n",
    "                c.active AS customer_active,\n",
    "                CONCAT(c.customer_phone_area,'-',c.customer_phone_number) AS customer_phone,\n",
    "                ab.is_target,\n",
    "                c.language AS customer_language,\n",
    "                o.delivery_address_district,\n",
    "                o.delivery_address_city,\n",
    "                o.delivery_address_state,\n",
    "                o.delivery_address_country,\n",
    "                o.merchant_id,\n",
    "                m.created_at AS merchant_created_at,\n",
    "                m.enabled AS merchant_enabled,\n",
    "                m.price_range,\n",
    "                m.average_ticket,\n",
    "                m.takeout_time,\n",
    "                m.delivery_time,\n",
    "                m.minimum_order_value,\n",
    "                m.merchant_city,\n",
    "                m.merchant_state,\n",
    "                IF(od.order_id IS NULL, FALSE, TRUE) has_details,\n",
    "                o.order_scheduled,\n",
    "                o.order_scheduled_date,\n",
    "                o.origin_platform,\n",
    "                CAST(DATE(DATE_TRUNC(o.order_created_at,week)) AS STRING) AS semana \n",
    "            FROM silver.order o\n",
    "            LEFT JOIN silver.consumer c \n",
    "                ON o.customer_id = c.customer_id\n",
    "            LEFT JOIN silver.merchants m \n",
    "                ON o.merchant_id = m.id\n",
    "            LEFT JOIN silver.ab_test ab \n",
    "                ON o.customer_id = ab.customer_id\n",
    "            LEFT JOIN silver.order_details od \n",
    "                ON  o.order_id = od.order_id \n",
    "                AND o.cpf = od.cpf\n",
    "        )\n",
    "        SELECT * EXCEPT(semana)\n",
    "        FROM tabela\n",
    "        WHERE semana = '{semana}'\n",
    "    \"\"\"\n",
    "\n",
    "    result = client.query(qry)\n",
    "    arrow_table = result.to_arrow()\n",
    "    df = pl.from_arrow(arrow_table)\n",
    "\n",
    "    # Salva parquet na pasta do projeto\n",
    "    nome_arquivo = f\"sales_{semana}.parquet\"\n",
    "    caminho_arquivo = os.path.join(pasta_projeto, nome_arquivo)\n",
    "    df.write_parquet(caminho_arquivo)\n",
    "\n",
    "    print(f\"Arquivo salvo: {caminho_arquivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del arrow_table\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros necessários para enviar os arquivos Parquet para o BigQuery\n",
    "dataset_nome = \"gold\"\n",
    "tabela_nome = \"sales\"\n",
    "project_id = \"case-ifood-fsg\"\n",
    "\n",
    "# Inserindo o DataFrame no BigQuery\n",
    "enviar_parquets_para_bigquery(pasta_projeto, dataset_nome, tabela_nome, client, project_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
