{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9c0f98",
   "metadata": {},
   "source": [
    "### Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "import io\n",
    "import tarfile\n",
    "import gc\n",
    "\n",
    "# Define o diretório para raiz dos arquivos\n",
    "diretorio = \"D:\\\\__case_ifood\"\n",
    "#  Define o caminho para a chave da conta de serviço\n",
    "service_account_path = os.path.join(diretorio,\"case-ifood-fsg-6f1d7cf34e08.json\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = service_account_path\n",
    "\n",
    "# Define o diretório para salvar os arquivos baixados do Bucket\n",
    "diretorio_arquivos_bucket = os.path.join(diretorio,\"bases_dados\")\n",
    "\n",
    "# Nome do bucket e do projeto\n",
    "bucket_name = \"case_ifood_fsg\"\n",
    "id_projeto = \"case-ifood-fsg\"\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "client_bq = bigquery.Client(project=id_projeto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea2f1a",
   "metadata": {},
   "source": [
    "### Função para salvar os dados no BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc265ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_dataset_e_tabela(df, dataset_nome, tabela_nome, client, project_id, location=\"southamerica-east1\"):\n",
    "    dataset_id = f\"{project_id}.{dataset_nome}\"\n",
    "    table_id = f\"{dataset_id}.{tabela_nome}\"\n",
    "\n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = location\n",
    "        client.create_dataset(dataset, exists_ok=True)\n",
    "        print(f\"Dataset '{dataset_id}' pronto.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = df.to_pandas()\n",
    "        print(\"Convertido de Polars para Pandas.\")\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"O objeto fornecido não é um DataFrame válido.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(df, table_id)\n",
    "        job.result()\n",
    "        print(f\"Tabela '{table_id}' criada com {job.output_rows} linhas.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar tabela: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360aaec9",
   "metadata": {},
   "source": [
    "### Carrega o arquivo restaurant.csv na camada Bronze do BQ como 'merchants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43819353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o nome do arquivo\n",
    "file_name_merchants = \"restaurant.csv.gz\" \n",
    "\n",
    "# Acessa o bucket e o blob (arquivo)\n",
    "bucket_merchants = client.bucket(bucket_name)\n",
    "blob_merchants = bucket_merchants.blob(file_name_merchants)\n",
    "\n",
    "# Faz o download como bytes\n",
    "data_bytes_merchants = blob_merchants.download_as_bytes()\n",
    "\n",
    "df_merchants = pd.read_csv(io.BytesIO(data_bytes_merchants), compression='gzip')\n",
    "\n",
    "# Visualiza as primeiras linhas\n",
    "df_merchants.head(3)\n",
    "\n",
    "# Insere os dados no BQ\n",
    "criar_dataset_e_tabela(\n",
    "    df=df_merchants,\n",
    "    dataset_nome=\"bronze\",\n",
    "    tabela_nome=\"merchants\",\n",
    "    client=client_bq,\n",
    "    project_id=id_projeto\n",
    ")\n",
    "\n",
    "del file_name_merchants\n",
    "del bucket_merchants\n",
    "del blob_merchants\n",
    "del data_bytes_merchants\n",
    "del df_merchants\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8d992",
   "metadata": {},
   "source": [
    "### Carrega o arquivo consumer.csv na camada Bronze do BQ como 'consumer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_consumer = \"consumer.csv.gz\" \n",
    "\n",
    "# Acessa o bucket e o blob (arquivo)\n",
    "bucket_consumer = client.bucket(bucket_name)\n",
    "blob_consumer = bucket_consumer.blob(file_name_consumer)\n",
    "\n",
    "# Faz o download como bytes\n",
    "data_bytes_consumer = blob_consumer.download_as_bytes()\n",
    "\n",
    "df_consumer = pd.read_csv(io.BytesIO(data_bytes_consumer), compression='gzip')\n",
    "\n",
    "# Visualiza as primeiras linhas\n",
    "print(df_consumer.head(3))\n",
    "\n",
    "# Insere os dados no BQ\n",
    "criar_dataset_e_tabela(\n",
    "    df=df_consumer,\n",
    "    dataset_nome=\"bronze\",\n",
    "    tabela_nome=\"consumer\",\n",
    "    client=client_bq,\n",
    "    project_id=id_projeto\n",
    ")\n",
    "\n",
    "del file_name_consumer\n",
    "del bucket_consumer\n",
    "del blob_consumer\n",
    "del data_bytes_consumer\n",
    "del df_consumer\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e5894",
   "metadata": {},
   "source": [
    "### Carrega o arquivo ab_test_ref.csv na camada Bronze do BQ como ab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_ab_test = \"ab_test_ref.tar.gz\" \n",
    "\n",
    "# Acessa o bucket e o blob (arquivo)\n",
    "bucket_ab_test = client.bucket(bucket_name)\n",
    "blob_ab_test = bucket_ab_test.blob(file_name_ab_test)\n",
    "\n",
    "# Faz o download como bytes\n",
    "data_bytes_ab_test = blob_ab_test.download_as_bytes()\n",
    "\n",
    "# Salva o arquivo na máquina local\n",
    "blob_ab_test.download_to_filename(os.path.join(diretorio_arquivos_bucket,file_name_ab_test))\n",
    "print(f\"Arquivo salvo em: {diretorio_arquivos_bucket}\")\n",
    "\n",
    "del bucket_ab_test\n",
    "del blob_ab_test\n",
    "del data_bytes_ab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e69351",
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_arquivo_leitura = \"ab_test_ref.csv\"\n",
    "\n",
    "# Extrai tudo\n",
    "with tarfile.open(os.path.join(diretorio_arquivos_bucket,file_name_ab_test), \"r:gz\") as tar:\n",
    "    tar.extractall(path=diretorio_arquivos_bucket)\n",
    "\n",
    "# Faz a leitura do arquivo que possui os dados\n",
    "df_ab_test = pd.read_csv(os.path.join(diretorio_arquivos_bucket,nome_arquivo_leitura), encoding=\"utf-8\")\n",
    "\n",
    "print(df_ab_test.head(3))\n",
    "\n",
    "# Insere os dados no BQ\n",
    "criar_dataset_e_tabela(\n",
    "    df=df_ab_test,\n",
    "    dataset_nome=\"bronze\",\n",
    "    tabela_nome=\"ab_test\",\n",
    "    client=client_bq,\n",
    "    project_id=id_projeto\n",
    ")\n",
    "\n",
    "del nome_arquivo_leitura\n",
    "del file_name_ab_test\n",
    "del df_ab_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100d29f",
   "metadata": {},
   "source": [
    "### Para o arquivo order.json.gz não foi possível fazer o insert com processamento local, por restrições de memória, com isso, foi criado a tabela diretamente no BQ na camada bronze, chamada orders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
