{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f60f19",
   "metadata": {},
   "source": [
    "## Configurações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import html # biblioteca para decodificar HTML\n",
    "import json # biblioteca para manipulação de JSON\n",
    "import gc # biblioteca para gerenciamento de memória\n",
    "import re # biblioteca para expressões regulares\n",
    "import glob # biblioteca para manipulação de arquivos globais\n",
    "import os # biblioteca para manipulação de sistema operacional\n",
    "\n",
    "pasta_projeto = \"D:\\\\__case_ifood\"\n",
    "\n",
    "# Caminho do arquivo JSON da conta de serviço\n",
    "credencial_gcp = os.path.join(pasta_projeto,\"case-ifood-fsg-6f1d7cf34e08.json\")\n",
    "\n",
    "# Autenticando\n",
    "credencial = service_account.Credentials.from_service_account_file(credencial_gcp)\n",
    "\n",
    "# Cliente BigQuery\n",
    "client = bigquery.Client(credentials=credencial, project=\"case-ifood-fsg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9425d18",
   "metadata": {},
   "source": [
    "### Função para criar o dataset e inserir uma tabela no BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size):\n",
    "    \"\"\"Divide um DataFrame Pandas em pedaços menores.\"\"\"\n",
    "    return [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "def criar_dataset_e_tabela(df, dataset_nome, tabela_nome, client, project_id, location=\"southamerica-east1\", chunk_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Cria um dataset (se necessário) e insere os dados em uma tabela BigQuery usando chunks.\n",
    "    Suporta DataFrames do Pandas ou Polars.\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud.exceptions import Conflict\n",
    "\n",
    "    dataset_id = f\"{project_id}.{dataset_nome}\"\n",
    "    table_id = f\"{dataset_id}.{tabela_nome}\"\n",
    "\n",
    "    # 1. Criar dataset se não existir\n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = location\n",
    "        client.create_dataset(dataset, exists_ok=True)\n",
    "        print(f\"Dataset '{dataset_id}' pronto.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Converter de Polars para Pandas, se necessário\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = df.to_pandas()\n",
    "        print(\"Convertido de Polars para Pandas.\")\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"O objeto fornecido não é um DataFrame válido.\")\n",
    "        return\n",
    "\n",
    "    # 3. Inserir em chunks\n",
    "    try:\n",
    "        chunks = split_dataframe(df, chunk_size)\n",
    "        total_rows = 0\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Enviando chunk {i+1}/{len(chunks)} com {len(chunk)} linhas...\")\n",
    "            write_disposition = (\n",
    "                bigquery.WriteDisposition.WRITE_TRUNCATE if i == 0\n",
    "                else bigquery.WriteDisposition.WRITE_APPEND\n",
    "            )\n",
    "            job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
    "            job = client.load_table_from_dataframe(chunk, table_id, job_config=job_config)\n",
    "            job.result()\n",
    "            total_rows += len(chunk)\n",
    "\n",
    "        print(f\"Tabela '{table_id}' carregada com {total_rows} linhas.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar tabela por chunks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a74ff6",
   "metadata": {},
   "source": [
    "## Transformando e inserindo os dados da Tabela merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "qry = \"SELECT * FROM bronze.merchants\"\n",
    "\n",
    "result = client.query(qry)\n",
    "arrow_table = result.to_arrow()\n",
    "df_merchants = pl.from_arrow(arrow_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb35798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merchants.head(5)  # Exibir as primeiras 5 linhas do DataFrame Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d76486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valida valores únicos, totais e nulos de merchants\n",
    "df_merchants.select(\n",
    "    pl.col(\"id\").n_unique().alias(\"total_unique_merchants\"),   # Merchants únicos\n",
    "    pl.col(\"id\").count().alias(\"total_merchants\"),                     # Total de linhas (merchants)\n",
    "    pl.col(\"id\").is_null().sum().alias(\"total_null_merchants\")# Merchants nulos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validando quantidade de merchants por status (enabled)\n",
    "(\n",
    "    df_merchants\n",
    "    .group_by(\"enabled\")\n",
    "    .agg([\n",
    "        pl.col(\"id\").n_unique().alias(\"total_unique_merchants\"),\n",
    "        pl.col(\"id\").count().alias(\"total_merchants\"),\n",
    "        pl.col(\"id\").is_null().sum().alias(\"total_null_merchants\")\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valida as top 10 cidades com mais parceiros únicos\n",
    "df_merchants.group_by(\"merchant_city\").agg([\n",
    "    pl.col(\"id\").n_unique().alias(\"total_unique_merchants\")\n",
    "]).sort(\"total_unique_merchants\", descending=True).head(10)  # Exibir os 10 maiores por cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo coluna para data e capitalizando nomes de cidades\n",
    "df_merchants = df_merchants.with_columns(\n",
    "    pl.col(\"created_at\").str.to_datetime(time_unit=\"ms\"),\n",
    "    pl.col(\"merchant_city\").str.to_titlecase()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserindo dados no BigQuery\n",
    "criar_dataset_e_tabela(\n",
    "    df=df_merchants,  \n",
    "    dataset_nome=\"silver\",\n",
    "    tabela_nome=\"merchants\",\n",
    "    client=client,\n",
    "    project_id=\"case-ifood-fsg\",\n",
    "    location=\"southamerica-east1\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eabbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_merchants  # Liberar memória\n",
    "del result  # Liberar memória\n",
    "del arrow_table  # Liberar memória\n",
    "gc.collect()  # Coletar lixo para liberar memória"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb118e45",
   "metadata": {},
   "source": [
    "## Transformando e inserindo os dados da tabela ab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_ab_test = \"SELECT * FROM bronze.ab_test\"\n",
    "\n",
    "result_ab_test = client.query(qry_ab_test)\n",
    "arrow_table_ab_test = result_ab_test.to_arrow()\n",
    "df_ab_test = pl.from_arrow(arrow_table_ab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ab_test.head(3)  # Exibir as primeiras 3 linhas do DataFrame Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validando se há somente cliente únicos no teste A/B\n",
    "df_ab_test.group_by(\"customer_id\").agg([\n",
    "    pl.col(\"customer_id\").count().alias(\"total_customers\"),\n",
    "    pl.col(\"customer_id\").n_unique().alias(\"total_unique_merchants\")\n",
    "]).sort(\"total_customers\", descending=True)  # Exibir os maiores por total de clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60383bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a porcentagem de clientes únicos no teste A/B\n",
    "df_ab_test.group_by(\"is_target\").agg([\n",
    "    pl.col(\"customer_id\").n_unique().alias(\"total_unique_customers\")\n",
    "]).with_columns([\n",
    "        pl.col(\"total_unique_customers\").sum().alias(\"total_customers\"),\n",
    "        (pl.col(\"total_unique_customers\") / pl.col(\"total_unique_customers\").sum()).round(3).alias(\"percentage_customers\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criar_dataset_e_tabela(\n",
    "    df=df_ab_test,  \n",
    "    dataset_nome=\"silver\",\n",
    "    tabela_nome=\"ab_test\",\n",
    "    client=client,\n",
    "    project_id=\"case-ifood-fsg\",\n",
    "    location=\"southamerica-east1\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ab_test # Liberar memória\n",
    "del result_ab_test  # Liberar memória\n",
    "del arrow_table_ab_test\n",
    "gc.collect()  # Coletar lixo para liberar memória"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437d695",
   "metadata": {},
   "source": [
    "## Transformando e inserindo os dados da tabela consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_consumer = \"SELECT * FROM bronze.consumer\"\n",
    "\n",
    "result_consumer = client.query(qry_consumer)\n",
    "arrow_table_consumer = result_consumer.to_arrow()\n",
    "df_consumer_polars = pl.from_arrow(arrow_table_consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer_polars.head(3)  # Exibir as primeiras 3 linhas do DataFrame Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42241c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer_polars.group_by(\"customer_id\").agg([\n",
    "    pl.col(\"customer_id\").count().alias(\"total_customers\"),\n",
    "    pl.col(\"customer_id\").n_unique().alias(\"total_unique_merchants\")\n",
    "]).sort(\"total_unique_merchants\", descending=True)  # Exibir os maiores por total de clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validando a porcentagem de clientes únicos por status e idioma\n",
    "df_consumer_polars.group_by([\"active\",\"language\"]).agg([\n",
    "    pl.col(\"customer_id\").n_unique().alias(\"total_unique_customers\"),\n",
    "    pl.col(\"customer_id\").count().alias(\"total_customers\")\n",
    "]).with_columns([\n",
    "    pl.col(\"total_customers\").sum().alias(\"total_customers_sum\"),\n",
    "    (pl.col(\"total_unique_customers\") / pl.col(\"total_customers\").sum()).round(3).alias(\"percentage_customers\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce45f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laço utilizado para remover aspas e desfazer entidades HTML\n",
    "\n",
    "df_consumer = df_consumer_polars.to_pandas()  # Convertendo de Polars para Pandas\n",
    "\n",
    "for col in df_consumer.select_dtypes(include='object'):\n",
    "    df_consumer[col] = (\n",
    "        df_consumer[col]\n",
    "        .str.replace('\"', '', regex=False)  # remove aspas duplas\n",
    "        .str.replace(\"'\", \"\", regex=False)  # remove aspas simples\n",
    "        .apply(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer[\"created_at\"] = pd.to_datetime(df_consumer[\"created_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "criar_dataset_e_tabela(\n",
    "    df=df_consumer,  \n",
    "    dataset_nome=\"silver\",\n",
    "    tabela_nome=\"consumer\",\n",
    "    client=client,\n",
    "    project_id=\"case-ifood-fsg\",\n",
    "    location=\"southamerica-east1\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_consumer_polars  # Liberar memória\n",
    "#del df_consumer # Liberar memória\n",
    "del result_consumer  # Liberar memória\n",
    "del arrow_table_consumer  # Liberar memória\n",
    "gc.collect()  # Coletar lixo para liberar memória"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b2387",
   "metadata": {},
   "source": [
    "## Transformando e inserindo os dados da tabela orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651ec00",
   "metadata": {},
   "source": [
    "#### Código abaixo utilizado para verificar a quantidade de registros por semana, pois tive dificuldades de ler toda a tabela diretamente\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "       CAST(\n",
    "          DATE(\n",
    "              DATE_TRUNC(o.order_created_at,WEEK)\n",
    "              ) \n",
    "         AS STRING) AS semana,\n",
    "       COUNT(*) qtd\n",
    "FROM `bronze.orders` o\n",
    "GROUP BY ALL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro criado para buscar dados mais rapidamente do BQ\n",
    "semanas = [\n",
    "    \"2018-12-02\", \"2018-12-09\", \"2018-12-16\", \"2018-12-23\",\n",
    "    \"2018-12-30\", \"2019-01-06\", \"2019-01-13\", \"2019-01-20\", \"2019-01-27\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_chunk = []\n",
    "\n",
    "for semana in semanas:\n",
    "    print(f\"Lendo semana: {semana}\")\n",
    "\n",
    "    qry = f\"\"\"\n",
    "        WITH base AS (\n",
    "            SELECT\n",
    "                CAST(DATE(DATE_TRUNC(order_created_at, WEEK)) AS STRING) AS semana,\n",
    "                * EXCEPT(items)\n",
    "            FROM bronze.orders\n",
    "        )\n",
    "        SELECT * EXCEPT(semana)\n",
    "        FROM base\n",
    "        WHERE semana = '{semana}'\n",
    "    \"\"\"\n",
    "\n",
    "    result = client.query(qry)\n",
    "    arrow_table = result.to_arrow()\n",
    "    df = pl.from_arrow(arrow_table)\n",
    "    df_orders_chunk.append(df)\n",
    "\n",
    "df_orders = pl.concat(df_orders_chunk, how=\"vertical\")\n",
    "\n",
    "print(\"Dados carregados com sucesso!\")\n",
    "print(df_orders.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df # Liberar memória\n",
    "del df_orders_chunk  # Liberar memória\n",
    "del arrow_table  # Liberar memória\n",
    "gc.collect()  # Coletar lixo para liberar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.head(3)  # Exibir as primeiras 3 linhas do DataFrame Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73481eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validando quantidade de pedidos únicos por cidade de entrega\n",
    "# Verificado que a coluna 'order_id' não é única por pedido, pois há pedidos com o mesmo ID em diferentes semanas\n",
    "df_orders.group_by(\"delivery_address_city\").agg([\n",
    "    pl.col(\"order_id\").n_unique().alias(\"total_unique_orders\"),\n",
    "    pl.col(\"order_id\").count().alias(\"total_orders\"),\n",
    "    pl.col(\"order_total_amount\").sum().alias(\"total_order_amount\")\n",
    "]).sort(\"total_unique_orders\", descending=True)  # Exibir os maiores por total de pedidos únicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para um mesmo pedido e mesmo cliente, tenho 2 ou mais registros,\n",
    "# Verificado que há mais de um cpf para o mesmo pedido\n",
    "df_orders.group_by([\"order_id\",\"customer_id\"]).agg([\n",
    "    pl.col(\"order_id\").count().alias(\"total_orders\"),\n",
    "    pl.col(\"cpf\").n_unique().alias(\"total_unique_cpf\") # Verifica se o CPF é único por pedido\n",
    "]).sort(\"total_orders\", descending=True).head(5)  # Exibir os maiores por total de pedidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a coluna 'rn' para identificar a ordem dos pedidos por cliente\n",
    "df_orders = (\n",
    "    df_orders.sort([\"order_id\", \"order_created_at\"])\n",
    "    .with_columns([\n",
    "        pl.arange(0, pl.len()).over(\"order_id\").alias(\"rn\") + 1\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d26071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.filter(pl.col(\"rn\") == 2 ).head(3)  # Exibir as primeiras 3 linhas do DataFrame Polars filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9506abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo duplicatas de pedidos por cliente\n",
    "df_orders = df_orders.filter(pl.col(\"rn\") == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a103f5",
   "metadata": {},
   "source": [
    "#### Criando, transformando e inserindo o df_order no BQ, que armazenará o cabeçalho do pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fc01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando algumas transformações\n",
    "df_orders = df_orders.with_columns(\n",
    "    pl.col(\"cpf\").cast(pl.Utf8).str.zfill(11), # preenchendo CPF com zeros à esquerda\n",
    "    pl.col(\"delivery_address_city\").str.to_titlecase(),\n",
    "    pl.col(\"customer_name\").str.to_titlecase().str.replace(\"'\", \"\"),\n",
    "    pl.col(\"delivery_address_district\").str.to_titlecase(),\n",
    ").select(\n",
    "    pl.col(\"order_id\"),\n",
    "    pl.col(\"order_created_at\"),\n",
    "    pl.col(\"cpf\"),\n",
    "    pl.col(\"customer_id\"),\n",
    "    pl.col(\"customer_name\"),\n",
    "    pl.col(\"delivery_address_district\"),\n",
    "    pl.col(\"delivery_address_city\"),\n",
    "    pl.col(\"delivery_address_state\"),\n",
    "    pl.col(\"delivery_address_country\"),\n",
    "    pl.col(\"delivery_address_zip_code\"),\n",
    "    pl.col(\"delivery_address_latitude\"),\n",
    "    pl.col(\"delivery_address_longitude\"),\n",
    "    pl.col(\"delivery_address_external_id\"),\n",
    "    pl.col(\"merchant_id\"),\n",
    "    pl.col(\"merchant_latitude\"),\n",
    "    pl.col(\"merchant_longitude\"),\n",
    "    pl.col(\"merchant_timezone\"),\n",
    "    pl.col(\"order_total_amount\"),\n",
    "    pl.col(\"order_scheduled\"),\n",
    "    pl.col(\"order_scheduled_date\"),\n",
    "    pl.col(\"origin_platform\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b53bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criar_dataset_e_tabela(\n",
    "    df=df_orders,  \n",
    "    dataset_nome=\"silver\",\n",
    "    tabela_nome=\"order\",\n",
    "    client=client,\n",
    "    project_id=\"case-ifood-fsg\",\n",
    "    location=\"southamerica-east1\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35673f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_orders  # Liberar memória\n",
    "gc.collect()  # Coletar lixo para liberar memória"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6425d",
   "metadata": {},
   "source": [
    "#### Criando, transformando e inserindo o df_order_details no BQ, que armazenará os itens do pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_order_details = []\n",
    "\n",
    "for semana in semanas:\n",
    "    print(f\"Lendo semana: {semana}\")\n",
    "# Como identificado na célula anterior, a coluna 'order_id' não é única por pedido,\n",
    "# então é necessário filtrar os pedidos para pegar os registros sem duplicatas.\n",
    "    qry = f\"\"\"\n",
    "        WITH base AS (\n",
    "                SELECT\n",
    "                    CAST(DATE(DATE_TRUNC(order_created_at, WEEK)) AS STRING) AS semana,\n",
    "                    order_id,\n",
    "                    cpf,\n",
    "                    items\n",
    "                FROM bronze.orders\n",
    "                QUALIFY ROW_NUMBER() OVER(PARTITION BY order_id ORDER BY order_created_at ASC) = 1\n",
    "            )\n",
    "                SELECT * EXCEPT(semana)\n",
    "                FROM base\n",
    "                WHERE semana = '{semana}'\n",
    "    \"\"\"\n",
    "\n",
    "    result = client.query(qry)\n",
    "    arrow_table = result.to_arrow()\n",
    "    df = pl.from_arrow(arrow_table)\n",
    "    dfs_order_details.append(df)\n",
    "\n",
    "df_order_details = pl.concat(dfs_order_details, how=\"vertical\")\n",
    "\n",
    "print(\"Dados carregados com sucesso!\")\n",
    "print(df_order_details.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df # Liberar memória\n",
    "del dfs_order_details  # Liberar memória\n",
    "del arrow_table  # Liberar memória\n",
    "gc.collect()  # Coletar lixo para liberar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula criada para buscar dados locais sem precisar do BQ\n",
    "#df_order_details.write_parquet(\n",
    " #   os.path.join(pasta_projeto,\"order_details.parquet\")\n",
    "#)\n",
    "\n",
    "#df_order_details = pl.read_parquet(os.path.join(pasta_projeto,\"order_details.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para limpar e analisar o JSON\n",
    "def safe_json_parse(text):\n",
    "    try:\n",
    "\n",
    "         # Garante que o texto é string\n",
    "        if not isinstance(text, str):\n",
    "            return None\n",
    "        \n",
    "        # Etapa 1: limpeza básica\n",
    "        text = text.strip()\n",
    "\n",
    "        # Remove aspas externas: \"....\" → ....\n",
    "        if text.startswith('\"') and text.endswith('\"'):\n",
    "            text = text[1:-1]\n",
    "\n",
    "        # Etapa 2: transforma \\\\\\\" → \" (escape duplo)\n",
    "        text = text.replace('\\\\\\\"', '\"')\n",
    "\n",
    "        # Etapa 3: transforma \\\" → \" (escape simples)\n",
    "        text = text.replace('\\\"', '\"')\n",
    "\n",
    "        # Etapa 4: substitui aspas duplas duplicadas no início e fim: \"\"abc\"\" → \"abc\"\n",
    "        text = re.sub(r'\"\"([^\"]*?)\"\"', r'\"\\1\"', text)\n",
    "\n",
    "        # Etapa 5: reduz excesso de aspas seguidas internas: \"\"\" → \"\n",
    "        text = re.sub(r'\"+', r'\"', text)\n",
    "\n",
    "        # Etapa 6: remove barras soltas antes de aspas\n",
    "        text = re.sub(r'\\\\\"', r'\"', text)\n",
    "\n",
    "        # Etapa 7: parse final\n",
    "        return json.loads(text)\n",
    "\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir o DataFrame em chunks para processamento\n",
    "chunk_size = 350_000\n",
    "n = df_order_details.height\n",
    "\n",
    "# Processar cada chunk\n",
    "for start in range(0, n, chunk_size):\n",
    "    end = min(start + chunk_size, n)\n",
    "    print(f\"Processando linhas {start} até {end}...\")\n",
    "\n",
    "    chunk = df_order_details.slice(start, end - start)\n",
    "    chunk = chunk.with_columns(\n",
    "        pl.col(\"items\").cast(pl.Utf8).str.strip_chars().alias(\"items_clean\")\n",
    "    )\n",
    "\n",
    "    chunk = chunk.with_columns(\n",
    "        pl.col(\"items_clean\").map_elements(safe_json_parse, return_dtype=pl.Object).alias(\"parsed_items\")\n",
    "    )\n",
    "\n",
    "# Filtrar os chunks válidos e inválidos\n",
    "    chunk_valid = chunk.filter(pl.col(\"parsed_items\").is_not_null())\n",
    "    chunk_invalid = chunk.filter(pl.col(\"parsed_items\").is_null())\n",
    "\n",
    "    # Salva as inválidas direto\n",
    "    if chunk_invalid.height > 0:\n",
    "        chunk_invalid = chunk_invalid.with_columns([\n",
    "        pl.col(c).cast(pl.Utf8) for c in chunk_invalid.columns\n",
    "    ])\n",
    "        chunk_invalid.write_csv(os.path.join(pasta_projeto,f\"invalid_{start}.csv\"))\n",
    "\n",
    "    # Explode manualmente\n",
    "    exploded_rows = []\n",
    "    for row in chunk_valid.iter_rows(named=True):\n",
    "        order_id = row[\"order_id\"]\n",
    "        cpf = row[\"cpf\"]\n",
    "        parsed_items = row[\"parsed_items\"]\n",
    "\n",
    "        if not isinstance(parsed_items, list):\n",
    "            continue\n",
    "\n",
    "        for item in parsed_items:\n",
    "            exploded_rows.append({\n",
    "                \"order_id\": order_id,\n",
    "                \"cpf\": cpf,\n",
    "                \"name\": str(item.get(\"name\", \"\")),\n",
    "                \"quantity\": str(item.get(\"quantity\", \"\")),\n",
    "                \"sequence\": str(item.get(\"sequence\", \"\")),\n",
    "                \"unitPrice\": str(item.get(\"unitPrice\", {}).get(\"value\", \"\")),\n",
    "                \"addition\": str(item.get(\"addition\", {}).get(\"value\", \"\")),\n",
    "                \"discount\": str(item.get(\"discount\", {}).get(\"value\", \"\")),\n",
    "                \"type\": \"principal\"\n",
    "            })\n",
    "\n",
    "            for g in item.get(\"garnishItems\", []):\n",
    "                exploded_rows.append({\n",
    "                    \"order_id\": order_id,\n",
    "                    \"cpf\": cpf,\n",
    "                    \"name\": str(g.get(\"name\", \"\")),\n",
    "                    \"quantity\": str(g.get(\"quantity\", \"\")),\n",
    "                    \"sequence\": str(g.get(\"sequence\", \"\")),\n",
    "                    \"unitPrice\": str(g.get(\"unitPrice\", {}).get(\"value\", \"\")),\n",
    "                    \"addition\": str(g.get(\"addition\", {}).get(\"value\", \"\")),\n",
    "                    \"discount\": str(g.get(\"discount\", {}).get(\"value\", \"\")),\n",
    "                    \"type\": \"garnish\"\n",
    "                })\n",
    "\n",
    "    # Salva o resultado do chunk direto\n",
    "    if exploded_rows:\n",
    "        df_chunk = pl.DataFrame(exploded_rows)\n",
    "        df_chunk.write_parquet(os.path.join(pasta_projeto,f\"chunk_{start}.parquet\"))\n",
    "        del df_chunk\n",
    "\n",
    "    # Limpa memória\n",
    "    del chunk, chunk_valid, chunk_invalid, exploded_rows\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Todos os chunks processados e salvos em disco com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_order_details # Liberar memória\n",
    "gc.collect()  # Coletar lixo para liberar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arquivos = glob.glob(os.path.join(pasta_projeto,\"chunk_*.parquet\"))\n",
    "dfs_parquet = [pl.read_parquet(arq) for arq in arquivos]\n",
    "df_order_details_explodido = pl.concat(dfs_parquet).sort(by=[\"order_id\", \"sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938db111",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfs_parquet # Liberar memória\n",
    "gc.collect() # Coletar lixo para liberar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_details_explodido = df_order_details_explodido.with_columns(\n",
    "    pl.col(\"cpf\").cast(pl.Utf8).str.zfill(11),\n",
    "    pl.col(\"name\").str.to_titlecase(),\n",
    "    pl.col(\"quantity\").cast(pl.Float64),\n",
    "    pl.col(\"sequence\").cast(pl.Int32),\n",
    "    pl.col(\"unitPrice\").cast(pl.Float64) / 100,\n",
    "    pl.col(\"addition\").cast(pl.Float64)  / 100,\n",
    "    pl.col(\"discount\").cast(pl.Float64)  / 100,\n",
    "    pl.col(\"type\").cast(pl.Utf8)\n",
    ").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "criar_dataset_e_tabela(\n",
    "    df=df_order_details_explodido,  \n",
    "    dataset_nome=\"silver\",\n",
    "    tabela_nome=\"order_details\",\n",
    "    client=client,\n",
    "    project_id=\"case-ifood-fsg\",\n",
    "    location=\"southamerica-east1\" \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
